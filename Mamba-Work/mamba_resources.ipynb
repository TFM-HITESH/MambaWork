{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Mamba 🐍\n",
    "Mamba is a new architecture designed for sequence modeling tasks in large language models (LLMs). It was developed to address some of the limitations of the popular transformer architecture, particularly when dealing with long sequences. Here's a breakdown of what makes Mamba unique:\n",
    "\n",
    "* **Focus on Selective State Spaces:** Unlike transformers that rely on attention mechanisms, Mamba uses selective state spaces (SSMs) to process information. These SSMs allow Mamba to filter out irrelevant data and focus on the important parts of a sequence, making it more efficient for handling long sequences.\n",
    "\n",
    "* **Hardware Friendly Design:** Mamba's architecture is designed to be parallel and run efficiently on modern hardware, especially GPUs. This translates to faster computations and lower memory requirements compared to traditional models.\n",
    "\n",
    "* **Simpler Structure:** By using SSMs and eliminating complex components like attention and MLP blocks, Mamba boasts a simpler overall structure. This simplicity contributes to better scalability and overall performance.\n",
    "\n",
    "Overall, Mamba offers several advantages over transformers, especially for tasks involving long sequences. It achieves faster inference, scales linearly with sequence length, and performs well on various modalities like language, audio, and even genomics data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources Related to Mamba ✍\n",
    "\n",
    "- Mamba Main Paper : https://arxiv.org/pdf/2312.00752\n",
    "- MoE Mamba : https://arxiv.org/pdf/2401.04081\n",
    "- Jamba Paper : https://arxiv.org/pdf/2403.19887\n",
    "- MambaByte : https://arxiv.org/pdf/2401.13660 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Based Resources 🔊\n",
    "> ##### Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation (2024.03.27)\n",
    "> Paper : https://arxiv.org/pdf/2403.18257\n",
    "\n",
    "> ##### SPMamba: State-space model is all you need in speech separation (2024.04.02)\n",
    "> Paper : https://arxiv.org/pdf/2404.02063 \\\n",
    "> Code : https://github.com/JusperLee/SPMamba \\\n",
    "> In this paper, we propose a network architecture for speech separation using a state-space model, namely SPMamba.\"\n",
    "\n",
    "> ##### SSAMBA: Self-Supervised Audio Representation Learning with Mamba State Space Model (2024.05.20)\n",
    "> Paper : https://arxiv.org/pdf/2405.11831 \\\n",
    ">\"We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification.\"\n",
    "\n",
    "> ##### Mamba in Speech: Towards an Alternative to Self-Attention (2024.05.21)\n",
    "> Paper : https://arxiv.org/pdf/2405.12609 \\\n",
    ">\"Mamba exhibited its effectiveness in natural language processing and\n",
    "computer vision tasks, but its superiority has rarely been investigated in speech\n",
    "signal processing.\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github Resources 🌐\n",
    "\n",
    "- Mamba Repo : https://github.com/state-spaces/mamba\n",
    "- Collection of Mamba Resources : https://github.com/XiudingCai/Awesome-Mamba-Collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources yet to look into\n",
    "\n",
    "> #### Install and Run First State Space Model - MambaChat\n",
    "> - YT Video : https://youtu.be/irc-QL93JJ4\n",
    "> - Github : https://github.com/redotvideo/mamba-chat\n",
    "\n",
    "> #### Efficiently Modeling Long Sequences with Structured State Spaces\n",
    "> - Implementation : https://srush.github.io/annotated-s4/\n",
    "\n",
    "> #### Jamba Implementation\n",
    "> - Article : https://www.ai21.com/blog/announcing-jamba\n",
    "> - Github Implementation Pytorch (Unofficial) : https://github.com/kyegomez/Jamba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look into :\n",
    "\n",
    "- Mamba in Speech: Towards an Alternative to\n",
    "Self-Attention : https://arxiv.org/pdf/2405.12609 : https://github.com/Tonyyouyou/Mamba-in-Speech\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
